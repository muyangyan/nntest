{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "d73dfc7e-9ce8-4b76-b269-8517a5ac4114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "    \n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "0c9a68a5-8007-401b-a245-7d09a0f05cd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "3498767b-91d2-4b96-8292-e086d2b73051",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_one_hot(x):\n",
    "    b = np.zeros((len(x),10))\n",
    "    for i in range(len(x)):\n",
    "        a = np.zeros(10)\n",
    "        a[x[i]] = 1\n",
    "        b[i] = a\n",
    "    return b\n",
    "data = np.array(data)\n",
    "test_data = data[0:999]\n",
    "train_data = data[1000:len(data)]\n",
    "train_labels_number = train_data[:, 0]\n",
    "train_labels = convert_one_hot(train_labels_number)\n",
    "train_layers = train_data[:, 1:train_data.shape[0]]/255\n",
    "test_labels_number = test_data[:, 0]\n",
    "test_labels = convert_one_hot(test_labels_number)\n",
    "test_layers = test_data[:, 1:test_data.shape[0]]/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "e5f68341-b2e8-4f22-a264-79bc10142374",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def d_sigmoid(x):\n",
    "    return(np.exp(x)/np.square(np.exp(x)+1) + 1e-15)\n",
    "\n",
    "def ReLU(x):\n",
    "    #return 1/(1 + np.exp(-x))\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def d_ReLU(x):\n",
    "    #return(np.exp(x)/np.square(np.exp(x)+1))\n",
    "    return x > 0\n",
    "\n",
    "def norm(x):\n",
    "    #print(np.max(x))\n",
    "    return x/(np.max(x) + 1e-15)\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x)/np.sum(np.exp(x))\n",
    "\n",
    "def ssd(y, p): #sum of squared differences\n",
    "    squared_diff = np.square(y - p)\n",
    "    return np.sum(squared_diff)\n",
    "\n",
    "def d_sd(y, p): #derivative of squared difference\n",
    "    return 2 * (y - p)\n",
    "\n",
    "def sum_cross_ent(y, p):\n",
    "    p += np.full(p.shape, 1e-15) #to prevent log(0)\n",
    "    return np.sum(-(y * np.log(p) + (1 - y) * np.log(1 - p)))\n",
    "\n",
    "def d_cross_ent(y, p):\n",
    "    p += np.full(p.shape, 1e-15) #to prevent division by zero\n",
    "    return ((1 - y) / (1 - p)) - (y / p)\n",
    "\n",
    "class Net: # accepts a tuple indicating the number of nodes in each layer. contains the weights array and biases vector for each layer\n",
    "\n",
    "    def __init__(self, layers): # must have atleast 2 layers (2 items in the layers tuple)\n",
    "        self.layers = layers #tuple\n",
    "        self.weights = []\n",
    "        for i in range(1, len(layers)):\n",
    "            self.weights.append(np.random.rand(layers[i],layers[i-1])/500)\n",
    "        self.biases = []\n",
    "        for i in range(1, len(layers)):\n",
    "            self.biases.append(np.random.rand(layers[i])/500)\n",
    "        self.net = [] # contains activations\n",
    "        for i in range(len(layers)):\n",
    "            self.net.append(np.zeros((layers[i])))\n",
    "\n",
    "    def dump(self):\n",
    "        print(\"DUMP========\")\n",
    "        print(self.biases)\n",
    "        print(self.weights)\n",
    "        print(self.net)\n",
    "        \n",
    "    def act(self, x):\n",
    "        return sigmoid(x)\n",
    "    \n",
    "    def d_act(self, x):\n",
    "        return d_sigmoid(x)\n",
    "        \n",
    "    def loss(self, expected, actual): #for one training example\n",
    "        #return sum_cross_ent(expected, actual)\n",
    "        return ssd(expected, actual)\n",
    "    \n",
    "    def d_loss(self, expected, actual): #for one training example\n",
    "        #return d_cross_ent(expected, actual)\n",
    "        return d_sd(expected, actual)\n",
    "    \n",
    "    def emp_loss(self, labels, inputs): #empirical loss, for whole testing set\n",
    "        loss_array = np.zeros((len(inputs)))\n",
    "        for i in range(len(inputs)):\n",
    "            output = self.forward(inputs[i])\n",
    "            loss_array[i] = self.loss(labels[i], output)\n",
    "        return np.mean(loss_array)\n",
    "    \n",
    "    def acc(self, labels_num, inputs):\n",
    "        successes = 0\n",
    "        for i in range(len(inputs)):\n",
    "            output = self.forward(inputs[i])\n",
    "            prediction = np.where(output == output.max())[0][0]\n",
    "            if prediction == labels_num[i]:\n",
    "                successes+=1\n",
    "        return successes/len(inputs)\n",
    "    \n",
    "    def forward(self, input_layer):\n",
    "        self.net[0] = input_layer\n",
    "        for i in range(1, len(self.net)):\n",
    "            self.net[i] = self.act(np.dot(self.weights[i-1], self.net[i-1]) + self.biases[i-1])\n",
    "        out = self.net[len(self.net)-1]\n",
    "        normalized = norm(out)\n",
    "        return softmax(normalized)\n",
    "    \n",
    "    def forprop(self, input_batch):\n",
    "        netlen = len(self.net)\n",
    "        layer = [0] * netlen\n",
    "        layer[0] = input_batch\n",
    "        for i in range(1, netlen):\n",
    "            TEMP = np.tile(self.biases[i-1], (len(layer[i-1]),1))\n",
    "            TEMP2 = np.dot(self.weights[i-1], layer[i-1].T).T\n",
    "            layer[i] = self.act(TEMP2 + TEMP)\n",
    "        layer[netlen-1] = np.apply_along_axis(norm,1,layer[netlen-1])\n",
    "        layer[netlen-1] = np.apply_along_axis(softmax,1,layer[netlen-1])\n",
    "        #print(\"layer\", layer[netlen-1])\n",
    "        return layer\n",
    "\n",
    "    def backprop(self, labels, batch): # for one batch\n",
    "        net = self.forprop(batch)\n",
    "        \n",
    "        d_w = []\n",
    "        for i in range(len(self.layers)-1):\n",
    "            d_w.append(np.zeros((self.layers[i+1],self.layers[i])))\n",
    "        d_b = []\n",
    "        for i in range(len(self.layers)-1):\n",
    "            d_b.append(np.zeros((self.layers[i+1])))\n",
    "        d_a = [] \n",
    "        for i in range(len(self.layers)):\n",
    "            d_a.append(np.zeros((self.layers[i])))\n",
    "            \n",
    "        for L in range(len(net[0])):\n",
    "            d_a[len(self.layers)-1] = self.d_loss(net[len(self.layers)-1][L], labels[L])\n",
    "            for i in reversed(range(len(self.layers)-1)):\n",
    "                z = np.dot(self.weights[i], net[i][L]) + self.biases[i]\n",
    "                TEMP0 = np.multiply(self.d_act(z), d_a[i+1])\n",
    "                TEMP1 = np.dot(TEMP0, self.weights[i])\n",
    "                d_a[i] = TEMP1/len(net[i+1][L])\n",
    "                d_b[i] += np.dot(self.d_act(z), d_a[i+1])/len(net[i+1][L])\n",
    "                d_w[i] += np.outer((d_a[i+1] * self.d_act(z)), net[i][L])\n",
    "                \n",
    "                #d_w[i] += (net[i][L] * self.d_act(z) * d_a[i+1])/L\n",
    "        \n",
    "        batch_size = len(batch)\n",
    "        for i in range(len(self.layers)-1):\n",
    "            d_b[i] /= batch_size\n",
    "            d_w[i] /= batch_size\n",
    "\n",
    "        '''\n",
    "        for i in reversed(range(len(self.layers)-1)):\n",
    "            for j in range(len(self.net[i])):\n",
    "                for k in range(len(self.net[i+1])):\n",
    "                    #calculates z for the whole layer (layer i)\n",
    "                    z_layer = np.dot(self.weights[i][k], net[i]) + self.biases[i][k]\n",
    "                    #calculates z for this specific node (layer i, index j)\n",
    "                    z = self.weights[i][k][j] * net[i][j] + self.biases[i][k]\n",
    "                    #calculates the suggested change \n",
    "                    d_activations[i][j] += (self.weights[i][k][j] * self.d_act(z_layer) * d_activations[i+1][k])/len(net[i+1])\n",
    "                    d_biases[i][k] += (self.d_act(z_layer) * d_activations[i+1][k])/len(net[i])\n",
    "                    d_weights[i][k][j] = net[i][j] * self.d_act(z) * d_activations[i+1][k]\n",
    "        '''\n",
    "        return d_w, d_b\n",
    "\n",
    "    def train(self, data, labels, batch_size, rate, *, batches=0): #labels and data are arrays of vectors (or 2d arrays), 1 epoch\n",
    "        #np.random.shuffle(data)\n",
    "        data_batched = np.zeros((math.floor(len(data)/batch_size), batch_size, self.layers[0]))\n",
    "        labels_batched = np.zeros((math.floor(len(data)/batch_size), batch_size, self.layers[len(self.layers) - 1]))\n",
    "        for i in range(math.floor(len(data)/batch_size)):\n",
    "            for j in range(batch_size):\n",
    "                data_batched[i][j] = data[i*batch_size + j]\n",
    "                labels_batched[i][j] = labels[i*batch_size + j]\n",
    "        \n",
    "        batch_count = batches\n",
    "        if (batches == 0):\n",
    "            batch_count = len(data_batched)\n",
    "\n",
    "        for i in range(len(data_batched)):\n",
    "            #self.dump()\n",
    "            \n",
    "            d_weights, d_biases = self.backprop(labels_batched[i], data_batched[i])\n",
    "                \n",
    "            for j in range(len(self.layers)-1):\n",
    "                self.weights[j] = np.subtract(self.weights[j], rate * d_weights[j])\n",
    "                self.weights[j] = self.weights[j]/np.linalg.norm(self.weights[j])\n",
    "                self.biases[j] = np.subtract(self.biases[j], rate * d_biases[j])\n",
    "                #self.biases[j] = self.biases[j]/np.linalg.norm(self.biases[j])\n",
    "            if i % 20 == 0:\n",
    "                print(\"\\nbatch\", i)\n",
    "                print(\"loss:\", self.emp_loss(test_labels, test_layers))\n",
    "                print(\"accuracy:\", self.acc(test_labels_number, test_layers))\n",
    "                #self.dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "61883ae1-0d69-41f5-86c1-5f45d648a5ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create net\n",
      "loss: 0.899995662339697\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train for one epoch\n",
      "\n",
      "batch 0\n",
      "loss: 0.9006745006623806\n",
      "accuracy: 0.12412412412412413\n",
      "\n",
      "batch 20\n",
      "loss: 0.9002340644598905\n",
      "accuracy: 0.12412412412412413\n",
      "\n",
      "batch 40\n",
      "loss: 0.8999114706697384\n",
      "accuracy: 0.12412412412412413\n",
      "\n",
      "batch 60\n",
      "loss: 0.8996172107848333\n",
      "accuracy: 0.12412412412412413\n",
      "\n",
      "batch 80\n",
      "loss: 0.8994540055281208\n",
      "accuracy: 0.12412412412412413\n",
      "\n",
      "batch 100\n",
      "loss: 0.8994990353835188\n",
      "accuracy: 0.12412412412412413\n",
      "\n",
      "batch 120\n",
      "loss: 0.899531080182587\n",
      "accuracy: 0.12412412412412413\n",
      "\n",
      "batch 140\n",
      "loss: 0.8997307196149247\n",
      "accuracy: 0.12412412412412413\n",
      "\n",
      "batch 160\n",
      "loss: 0.8997576538761362\n",
      "accuracy: 0.12412412412412413\n",
      "\n",
      "batch 180\n",
      "loss: 0.8997972546844847\n",
      "accuracy: 0.12412412412412413\n",
      "\n",
      "batch 200\n",
      "loss: 0.8997348170702495\n",
      "accuracy: 0.13813813813813813\n",
      "\n",
      "batch 220\n",
      "loss: 0.8997531322573897\n",
      "accuracy: 0.12412412412412413\n",
      "\n",
      "batch 240\n",
      "loss: 0.8996814809213247\n",
      "accuracy: 0.12412412412412413\n",
      "\n",
      "batch 260\n",
      "loss: 0.8996625227940607\n",
      "accuracy: 0.12412412412412413\n",
      "\n",
      "batch 280\n",
      "loss: 0.8996541260663978\n",
      "accuracy: 0.10510510510510511\n",
      "\n",
      "batch 300\n",
      "loss: 0.8996962228847651\n",
      "accuracy: 0.10510510510510511\n",
      "\n",
      "batch 320\n",
      "loss: 0.8997636611718378\n",
      "accuracy: 0.11511511511511512\n",
      "\n",
      "batch 340\n",
      "loss: 0.8998994571062523\n",
      "accuracy: 0.10510510510510511\n",
      "\n",
      "batch 360\n",
      "loss: 0.8998971185725164\n",
      "accuracy: 0.10510510510510511\n",
      "\n",
      "batch 380\n",
      "loss: 0.8999778381270457\n",
      "accuracy: 0.09009009009009009\n",
      "\n",
      "batch 400\n",
      "loss: 0.8998894055464576\n",
      "accuracy: 0.10710710710710711\n"
     ]
    }
   ],
   "source": [
    "print(\"create net\")\n",
    "testnet = Net((784, 20, 10))\n",
    "print(\"loss:\", testnet.emp_loss(test_labels, test_layers))\n",
    "#print(\"net created, calculating initial loss\")\n",
    "#print(testnet.loss(test_layers, test_labels))\n",
    "#testnet.dump()\n",
    "\n",
    "img0 = np.array(data[0][1:785])\n",
    "plt.imshow(np.reshape(img0, (28,28)))\n",
    "plt.show()\n",
    "#plt.imshow(train_imgs[1])\n",
    "#plt.show()\n",
    "#plt.imshow(train_imgs[2])\n",
    "#plt.show()\n",
    "#print(testnet.forward(data[0][1:785]))\n",
    "#print(testnet.forward(train_input_layers[1]))\n",
    "#print(testnet.forward(train_input_layers[2]))\n",
    "\n",
    "print(\"train for one epoch\")\n",
    "testnet.train(train_layers, train_labels, 100, 0.1, batches=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc116a5-6573-41b8-9af6-462943add735",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d09338-6476-4830-945c-78ad36d8d097",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
